{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\Tiki\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Users\\Tiki\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Users\\Tiki\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\n    return importlib.import_module('_pywrap_tensorflow_internal')\n  File \"C:\\Users\\Tiki\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 37, in import_module\n    __import__(name)\nImportError: No module named _pywrap_tensorflow_internal\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5bcab48aae42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tiki\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tiki\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tiki\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 52\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\Tiki\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Users\\Tiki\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Users\\Tiki\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\n    return importlib.import_module('_pywrap_tensorflow_internal')\n  File \"C:\\Users\\Tiki\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 37, in import_module\n    __import__(name)\nImportError: No module named _pywrap_tensorflow_internal\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0]) # return an integer\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o', 'w', 'l', ' ', 'm', 'h', 'y', 'a', 't', 'm', 'n', 'h', 'e', 'e', 'o', 'y', 'o', 'a', ' ', 'a', 'i', ' ', 't', 'd', 'f', 'a', 'e', 'e', 'a', 'r', 'i', 'o', 'a', 'g', 'i', 'r', 'c', 'a', ' ', 'm', 't', 'u', 'e', 'o', 'o', 's', 'k', 'e', 'w', 'e', 't', 'e', ' ', 'i', 't', 'd', 't', 'e', 'f', 'd', 't', 'a', 'a', 's']\n",
      "['on', 'wh', 'll', ' a', 'ma', 'he', 'y ', 'ay', 'ti', 'mi', 'ne', 'he', 'e ', 'eb', 'o ', 'ye', 'or', 'a ', ' t', 'ar', 'it', ' a', 'ti', 'dy', 'f ', 'at', 'e ', 'en', 'am', 'rv', 'io', 'o ', 'a ', 'gh', 'in', 'ro', 'ca', 'as', ' d', 'mo', 't ', 'u ', 'e ', 'o ', 'of', 's ', 'kl', 'er', 'ws', 'et', 'th', 'et', ' s', 'is', 'ti', 'd ', 'th', 'en', 'fe', 'du', 'tr', 'at', 'ap', 'si']\n",
      "['ons', 'whe', 'lle', ' ab', 'mar', 'hel', 'y a', 'ay ', 'tio', 'mig', 'new', 'he ', 'e l', 'ebe', 'o b', 'yer', 'ore', 'a f', ' tw', 'ari', 'ity', ' an', 'tio', 'dy ', 'f c', 'at ', 'e c', 'ent', 'amp', 'rve', 'iou', 'o c', 'a d', 'gh ', 'ine', 'ros', 'cal', 'ast', ' di', 'mos', 't s', 'u i', 'e o', 'o e', 'of ', 's t', 'kla', 'erp', 'ws ', 'et ', 'the', 'etc', ' sh', 'ise', 'tin', 'd n', 'th ', 'enc', 'fen', 'dua', 'tre', 'ati', 'app', 'si ']\n",
      "['ons ', 'when', 'ller', ' abb', 'marr', 'hel ', 'y an', 'ay o', 'tion', 'migr', 'new ', 'he b', 'e li', 'eber', 'o be', 'yer ', 'ore ', 'a fi', ' two', 'aris', 'ity ', ' and', 'tion', 'dy t', 'f ce', 'at i', 'e co', 'ent ', 'ampa', 'rver', 'ious', 'o ca', 'a du', 'gh a', 'ine ', 'ross', 'cal ', 'ast ', ' dim', 'most', 't s ', 'u is', 'e os', 'o ei', 'of i', 's th', 'klah', 'erpr', 'ws b', 'et i', 'the ', 'etch', ' sha', 'ised', 'ting', 'd ne', 'th r', 'ency', 'fens', 'duat', 'tree', 'atio', 'appe', 'si h']\n",
      "['ons a', 'when ', 'lleri', ' abbe', 'marri', 'hel a', 'y and', 'ay op', 'tion ', 'migra', 'new y', 'he bo', 'e lis', 'eber ', 'o be ', 'yer w', 'ore s', 'a fie', ' two ', 'arist', 'ity c', ' and ', 'tion ', 'dy to', 'f cer', 'at it', 'e con', 'ent t', 'ampai', 'rver ', 'ious ', 'o cap', 'a dup', 'gh an', 'ine j', 'ross ', 'cal t', 'ast i', ' dime', 'most ', 't s s', 'u is ', 'e osc', 'o eig', 'of it', 's the', 'klaho', 'erpri', 'ws be', 'et in', 'the f', 'etchy', ' shar', 'ised ', 'ting ', 'd neo', 'th ri', 'encyc', 'fense', 'duati', 'treet', 'ation', 'appea', 'si ha']\n",
      "['ons an', 'when m', 'lleria', ' abbey', 'marrie', 'hel an', 'y and ', 'ay ope', 'tion f', 'migrat', 'new yo', 'he boe', 'e list', 'eber h', 'o be m', 'yer wh', 'ore si', 'a fier', ' two s', 'aristo', 'ity ca', ' and i', 'tion o', 'dy to ', 'f cert', 'at it ', 'e conv', 'ent to', 'ampaig', 'rver s', 'ious t', 'o capi', 'a dupl', 'gh ann', 'ine ja', 'ross z', 'cal th', 'ast in', ' dimen', 'most h', 't s su', 'u is s', 'e osci', 'o eigh', 'of ita', 's the ', 'klahom', 'erpris', 'ws bec', 'et in ', 'the fa', 'etchy ', ' sharm', 'ised e', 'ting i', 'd neo ', 'th ris', 'encycl', 'fense ', 'duatin', 'treet ', 'ations', 'appeal', 'si hav']\n",
      "['ons ana', 'when mi', 'lleria ', ' abbeys', 'married', 'hel and', 'y and l', 'ay open', 'tion fr', 'migrati', 'new yor', 'he boei', 'e liste', 'eber ha', 'o be ma', 'yer who', 'ore sig', 'a fierc', ' two si', 'aristot', 'ity can', ' and in', 'tion of', 'dy to p', 'f certa', 'at it w', 'e convi', 'ent tol', 'ampaign', 'rver si', 'ious te', 'o capit', 'a dupli', 'gh ann ', 'ine jan', 'ross ze', 'cal the', 'ast ins', ' dimens', 'most ho', 't s sup', 'u is st', 'e oscil', 'o eight', 'of ital', 's the t', 'klahoma', 'erprise', 'ws beco', 'et in a', 'the fab', 'etchy t', ' sharma', 'ised em', 'ting in', 'd neo l', 'th risk', 'encyclo', 'fense t', 'duating', 'treet g', 'ations ', 'appeal ', 'si have']\n",
      "['ons anar', 'when mil', 'lleria a', ' abbeys ', 'married ', 'hel and ', 'y and li', 'ay opene', 'tion fro', 'migratio', 'new york', 'he boein', 'e listed', 'eber has', 'o be mad', 'yer who ', 'ore sign', 'a fierce', ' two six', 'aristotl', 'ity can ', ' and int', 'tion of ', 'dy to pa', 'f certai', 'at it wi', 'e convin', 'ent told', 'ampaign ', 'rver sid', 'ious tex', 'o capita', 'a duplic', 'gh ann e', 'ine janu', 'ross zer', 'cal theo', 'ast inst', ' dimensi', 'most hol', 't s supp', 'u is sti', 'e oscill', 'o eight ', 'of italy', 's the to', 'klahoma ', 'erprise ', 'ws becom', 'et in a ', 'the fabi', 'etchy to', ' sharman', 'ised emp', 'ting in ', 'd neo la', 'th risky', 'encyclop', 'fense th', 'duating ', 'treet gr', 'ations m', 'appeal o', 'si have ']\n",
      "['ons anarc', 'when mili', 'lleria ar', ' abbeys a', 'married u', 'hel and r', 'y and lit', 'ay opened', 'tion from', 'migration', 'new york ', 'he boeing', 'e listed ', 'eber has ', 'o be made', 'yer who r', 'ore signi', 'a fierce ', ' two six ', 'aristotle', 'ity can b', ' and intr', 'tion of t', 'dy to pas', 'f certain', 'at it wil', 'e convinc', 'ent told ', 'ampaign a', 'rver side', 'ious text', 'o capital', 'a duplica', 'gh ann es', 'ine janua', 'ross zero', 'cal theor', 'ast insta', ' dimensio', 'most holy', 't s suppo', 'u is stil', 'e oscilla', 'o eight s', 'of italy ', 's the tow', 'klahoma p', 'erprise l', 'ws become', 'et in a n', 'the fabia', 'etchy to ', ' sharman ', 'ised empe', 'ting in p', 'd neo lat', 'th risky ', 'encyclope', 'fense the', 'duating f', 'treet gri', 'ations mo', 'appeal of', 'si have m']\n",
      "['ons anarch', 'when milit', 'lleria arc', ' abbeys an', 'married ur', 'hel and ri', 'y and litu', 'ay opened ', 'tion from ', 'migration ', 'new york o', 'he boeing ', 'e listed w', 'eber has p', 'o be made ', 'yer who re', 'ore signif', 'a fierce c', ' two six e', 'aristotle ', 'ity can be', ' and intra', 'tion of th', 'dy to pass', 'f certain ', 'at it will', 'e convince', 'ent told h', 'ampaign an', 'rver side ', 'ious texts', 'o capitali', 'a duplicat', 'gh ann es ', 'ine januar', 'ross zero ', 'cal theori', 'ast instan', ' dimension', 'most holy ', 't s suppor', 'u is still', 'e oscillat', 'o eight su', 'of italy l', 's the towe', 'klahoma pr', 'erprise li', 'ws becomes', 'et in a na', 'the fabian', 'etchy to r', ' sharman n', 'ised emper', 'ting in po', 'd neo lati', 'th risky r', 'encycloped', 'fense the ', 'duating fr', 'treet grid', 'ations mor', 'appeal of ', 'si have ma']\n",
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['i', 'a', 'h', 'd', 'r', 'c', 'r', 'f', 't', 't', 't', 's', 'i', 'r', 't', 'c', 'i', 'r', 'i', 's', ' ', 'c', 'e', ' ', 'd', ' ', ' ', 'i', 'd', 's', ' ', 'z', 'e', 'd', 'y', 't', 'e', 'c', 'a', 'm', 't', ' ', 'i', 'b', 'a', 'r', 'e', 'n', ' ', 'z', ' ', 'e', 'e', 'o', 'l', 'n', 'i', 'i', 'a', 'o', ' ', 'e', 'd', 'd']\n",
      "['is', 'ar', 'he', 'd ', 'ra', 'ch', 'rg', 'fo', 'th', 'to', 'th', 'se', 'it', 'ro', 'to', 'ce', 'ic', 'ri', 'ig', 's ', ' l', 'ce', 'e ', ' h', 'dr', ' t', ' t', 'im', 'd ', 'st', ' s', 'ze', 'e ', 'd ', 'y ', 'th', 'es', 'ce', 'al', 'mo', 't ', ' d', 'in', 'bt', 'an', 'r ', 'es', 'nu', ' t', 'zi', ' s', 'el', 'et', 'or', 'li', 'n ', 'is', 'ic', 'ai', 'om', ' c', 'e ', 'de', 'de']\n",
      "['ist', 'ary', 'hes', 'd m', 'rac', 'cha', 'rgi', 'for', 'the', 'too', 'the', 'sev', 'ith', 'rob', 'to ', 'cei', 'ica', 'rit', 'igh', 's u', ' lo', 'cel', 'e s', ' hi', 'dru', ' ta', ' th', 'im ', 'd b', 'sta', ' su', 'ze ', 'e o', 'd h', 'y e', 'the', 'es ', 'ce ', 'al ', 'mor', 't o', ' di', 'ing', 'bty', 'ang', 'r c', 'ess', 'nux', ' th', 'zi ', ' so', 'ela', 'etw', 'or ', 'lit', 'n m', 'isk', 'ic ', 'air', 'om ', ' ce', 'e t', 'dev', 'de ']\n",
      "['ists', 'ary ', 'hes ', 'd mo', 'raca', 'char', 'rgic', 'for ', 'the ', 'took', 'ther', 'seve', 'ith ', 'roba', 'to r', 'ceiv', 'ican', 'riti', 'ight', 's un', ' los', 'cell', 'e si', ' him', 'drug', ' tak', ' the', 'im t', 'd ba', 'stan', ' suc', 'ze o', 'e of', 'd hi', 'y ei', 'the ', 'es c', 'ce t', 'al a', 'morm', 't or', ' dis', 'ing ', 'btyp', 'angu', 'r co', 'ess ', 'nux ', ' the', 'zi c', ' soc', 'elat', 'etwo', 'or h', 'liti', 'n mo', 'iske', 'ic o', 'air ', 'om a', ' cen', 'e th', 'devo', 'de s']\n",
      "['ists ', 'ary g', 'hes n', 'd mon', 'raca ', 'chard', 'rgica', 'for p', 'the n', 'took ', 'ther ', 'seven', 'ith a', 'robab', 'to re', 'ceive', 'icant', 'ritic', 'ight ', 's unc', ' lost', 'cellu', 'e siz', ' him ', 'drugs', ' take', ' the ', 'im to', 'd bar', 'stand', ' such', 'ze on', 'e of ', 'd hiv', 'y eig', 'the l', 'es cl', 'ce th', 'al an', 'mormo', 't or ', ' disa', 'ing s', 'btype', 'angua', 'r com', 'ess o', 'nux s', ' the ', 'zi co', ' soci', 'elati', 'etwor', 'or hi', 'litic', 'n mos', 'isker', 'ic ov', 'air c', 'om ac', ' cent', 'e tha', 'devot', 'de su']\n",
      "['ists a', 'ary go', 'hes na', 'd mona', 'raca p', 'chard ', 'rgical', 'for pa', 'the na', 'took p', 'ther w', 'seven ', 'ith a ', 'robabl', 'to rec', 'ceived', 'icant ', 'ritic ', 'ight i', 's unca', ' lost ', 'cellul', 'e size', ' him a', 'drugs ', ' take ', ' the p', 'im to ', 'd barr', 'standa', ' such ', 'ze on ', 'e of t', 'd hive', 'y eigh', 'the le', 'es cla', 'ce the', 'al ana', 'mormon', 't or a', ' disag', 'ing sy', 'btypes', 'anguag', 'r comm', 'ess on', 'nux su', ' the f', 'zi con', ' socie', 'elativ', 'etwork', 'or hir', 'litica', 'n most', 'iskerd', 'ic ove', 'air co', 'om acn', ' cente', 'e than', 'devoti', 'de suc']\n",
      "['ists ad', 'ary gov', 'hes nat', 'd monas', 'raca pr', 'chard b', 'rgical ', 'for pas', 'the nat', 'took pl', 'ther we', 'seven s', 'ith a g', 'robably', 'to reco', 'ceived ', 'icant t', 'ritic o', 'ight in', 's uncau', ' lost a', 'cellula', 'e size ', ' him a ', 'drugs c', ' take t', ' the pr', 'im to n', 'd barre', 'standar', ' such a', 'ze on t', 'e of th', 'd hiver', 'y eight', 'the lea', 'es clas', 'ce the ', 'al anal', 'mormons', 't or at', ' disagr', 'ing sys', 'btypes ', 'anguage', 'r commi', 'ess one', 'nux sus', ' the fi', 'zi conc', ' societ', 'elative', 'etworks', 'or hiro', 'litical', 'n most ', 'iskerdo', 'ic over', 'air com', 'om acnm', ' center', 'e than ', 'devotio', 'de such']\n",
      "['ists adv', 'ary gove', 'hes nati', 'd monast', 'raca pri', 'chard ba', 'rgical l', 'for pass', 'the nati', 'took pla', 'ther wel', 'seven si', 'ith a gl', 'robably ', 'to recog', 'ceived t', 'icant th', 'ritic of', 'ight in ', 's uncaus', ' lost as', 'cellular', 'e size o', ' him a s', 'drugs co', ' take to', ' the pri', 'im to na', 'd barred', 'standard', ' such as', 'ze on th', 'e of the', 'd hiver ', 'y eight ', 'the lead', 'es class', 'ce the n', 'al analy', 'mormons ', 't or at ', ' disagre', 'ing syst', 'btypes b', 'anguages', 'r commis', 'ess one ', 'nux suse', ' the fir', 'zi conce', ' society', 'elativel', 'etworks ', 'or hiroh', 'litical ', 'n most o', 'iskerdoo', 'ic overv', 'air comp', 'om acnm ', ' centerl', 'e than a', 'devotion', 'de such ']\n",
      "['ists advo', 'ary gover', 'hes natio', 'd monaste', 'raca prin', 'chard bae', 'rgical la', 'for passe', 'the natio', 'took plac', 'ther well', 'seven six', 'ith a glo', 'robably b', 'to recogn', 'ceived th', 'icant tha', 'ritic of ', 'ight in s', 's uncause', ' lost as ', 'cellular ', 'e size of', ' him a st', 'drugs con', ' take to ', ' the prie', 'im to nam', 'd barred ', 'standard ', ' such as ', 'ze on the', 'e of the ', 'd hiver o', 'y eight m', 'the lead ', 'es classi', 'ce the no', 'al analys', 'mormons b', 't or at l', ' disagree', 'ing syste', 'btypes ba', 'anguages ', 'r commiss', 'ess one n', 'nux suse ', ' the firs', 'zi concen', ' society ', 'elatively', 'etworks s', 'or hirohi', 'litical i', 'n most of', 'iskerdoo ', 'ic overvi', 'air compo', 'om acnm a', ' centerli', 'e than an', 'devotiona', 'de such d']\n",
      "['ists advoc', 'ary govern', 'hes nation', 'd monaster', 'raca princ', 'chard baer', 'rgical lan', 'for passen', 'the nation', 'took place', 'ther well ', 'seven six ', 'ith a glos', 'robably be', 'to recogni', 'ceived the', 'icant than', 'ritic of t', 'ight in si', 's uncaused', ' lost as i', 'cellular i', 'e size of ', ' him a sti', 'drugs conf', ' take to c', ' the pries', 'im to name', 'd barred a', 'standard f', ' such as e', 'ze on the ', 'e of the o', 'd hiver on', 'y eight ma', 'the lead c', 'es classic', 'ce the non', 'al analysi', 'mormons be', 't or at le', ' disagreed', 'ing system', 'btypes bas', 'anguages t', 'r commissi', 'ess one ni', 'nux suse l', ' the first', 'zi concent', ' society n', 'elatively ', 'etworks sh', 'or hirohit', 'litical in', 'n most of ', 'iskerdoo r', 'ic overvie', 'air compon', 'om acnm ac', ' centerlin', 'e than any', 'devotional', 'de such de']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' ']\n",
      "[' a']\n",
      "[' a']\n",
      "['a']\n",
      "['an']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      # the consecutive characters in the batch are segment away in the text\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))] # join individual character together\n",
    "    print(s)\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch. \n",
    "  It is cross-entropy, the smaller the better, if predictions are itendital\n",
    "  to labels, the result is zero.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element based on probabilities from a distribution \n",
    "  assumed to be an array of normalized probabilities.\"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  # train_data is a list of tensor\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "            \n",
    "    '''tf.control_dependencies function makes TensorFlow execute\n",
    "    certain operations before any other operations defined inside'\n",
    "    of the ‘with’ block. '''\n",
    "    \n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, decay_steps = 5000, decay_rate = 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss)) # returns gradient for variable\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295490 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.99\n",
      "================================================================================\n",
      "f s mulaoumcnnjtrvgcxna uapqak dy optt  o iliful hvw vzuef w ebbctkxoe rttbhoenf\n",
      "vgre bjdla  ebo bgxjntx wipgrtqmreiapzwsu  h eack tcgj gg a nttamsttp mhyhwn rtl\n",
      "ewkb dh yb c uvlhoudljiw s buysppipnyhdzzn nws  jbbmsoaivts v t  hifar ccowtdyiz\n",
      "uxe gm parut tdxe  iamknar  kxqvyeppkw h  nrr xgpi polge u tg cgtdkrzcmfyu  jppa\n",
      "gwe   piog  xv  iotggifmvksbho xetxtjv tig ce sooa  la nvrgb ipt ad h xldafle dt\n",
      "================================================================================\n",
      "Validation set perplexity: 20.22\n",
      "Average loss at step 100: 2.586869 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.91\n",
      "Validation set perplexity: 10.31\n",
      "Average loss at step 200: 2.241697 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.37\n",
      "Validation set perplexity: 8.45\n",
      "Average loss at step 300: 2.095622 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 400: 1.999932 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.50\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 500: 1.935975 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 600: 1.908430 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 700: 1.864047 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 800: 1.822481 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 900: 1.829906 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.07\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 1000: 1.826135 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "================================================================================\n",
      "b father evel of the resided from exse nothenia the taciph as dismece of meght b\n",
      "z hf other the nine the perin prear drant is of the were wale ne is mand of the \n",
      "n comouts duquaid in one onglate dispoition to was and dismels is the fieh to sa\n",
      "fon catly ater with enery has befer of the guture wene nishuenulicare in utled h\n",
      "ties his abods of hins the sine vero six themed from greec the sowuciat ormaties\n",
      "================================================================================\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1100: 1.780924 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1200: 1.752714 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1300: 1.735204 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1400: 1.746164 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1500: 1.734287 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1600: 1.745795 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1700: 1.715758 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1800: 1.672722 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 1900: 1.644099 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2000: 1.696436 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "================================================================================\n",
      "trist for empection compositions fumplist can ledv somption tembers to modew and\n",
      "form yearsthad fundea than ranghisly lopuly or simul by pat exferss of the alchu\n",
      "z appares wor spetic aasingenta line commbytical feath by bingas too choscome wa\n",
      "ing alpesibless lave contument to longer his duinchosary at zebrance they to spe\n",
      "ore gonard desuibegnous one nine eight five his curty singbels souncide are is t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2100: 1.683349 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 2200: 1.677327 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2300: 1.638365 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2400: 1.657055 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 2500: 1.677496 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 2600: 1.657059 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2700: 1.655449 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2800: 1.648727 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2900: 1.649471 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3000: 1.648179 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "rmen in soverticned to is also a coustege carry carle of by nolwed the four one \n",
      "he randan miswibld cates or difter lattey ariatible bundennes are also sets same\n",
      "n hister and steats altivind consection from the newaude the rass postionist rec\n",
      "joets hejosporties from however form boloring with rapandry addxain mich with b \n",
      "s in blandtell indian and  rovitthoric combical gautleer day and thes up arnal i\n",
      "================================================================================\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3100: 1.628745 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3200: 1.641995 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3300: 1.634973 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3400: 1.666922 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3500: 1.652139 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3600: 1.666100 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3700: 1.644635 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3800: 1.641087 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3900: 1.634311 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4000: 1.652232 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "r malda god trease phoced generally maira of chelugy standar chezopet and mamas \n",
      "former in cambed not the grouged the most centing efflges may god mach is popena\n",
      "rip althencation the melt doinketer of gively has wai eri of gunantifal known th\n",
      "former on was alecte romougholaca in clound mida apricic hu divines contemps and\n",
      "ter had the hadinction durce musemos faaf that vicha divictions and laters of go\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4100: 1.630920 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4200: 1.630759 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4300: 1.614792 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4400: 1.607702 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4500: 1.610047 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4600: 1.611471 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4700: 1.619727 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4800: 1.626227 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4900: 1.628088 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5000: 1.603805 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "================================================================================\n",
      "population emplasion of the supposity and which divisions dispriling basee preco\n",
      "leg aalon zero one eight five seven six one nine five gno zero zeoot see is wust\n",
      "jele and lativical and into the not four bome off thi contine aalan piceles befo\n",
      "quamaa ethinga eight s wealle theory two euter hunodue countles players jopteeno\n",
      "qu born by kale legon best with the havement of the laviston a the pared these m\n",
      "================================================================================\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5100: 1.602075 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5200: 1.589324 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5300: 1.574192 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5400: 1.575222 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5500: 1.560975 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5600: 1.581051 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5700: 1.568108 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5800: 1.580601 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5900: 1.573031 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6000: 1.543847 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "y azits oppaine since ivshan strings and dertonid capss aububated weut asmerffat\n",
      "hard one zero forcz from they mustord fikmetic ip an his position who the cairor\n",
      "hist in one nine marofaa s known of the almasonest propsnan bank time was vietik\n",
      "urally begins are cline constriting anguiles syiticssern john one three the true\n",
      "notee of from this pouny nateemed a have is everchen defaikn but is from lays ko\n",
      "================================================================================\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6100: 1.565650 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6200: 1.537201 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6300: 1.542676 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6400: 1.537652 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6500: 1.557002 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6600: 1.592173 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6700: 1.573042 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6800: 1.603734 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6900: 1.573825 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 7000: 1.571343 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "ar of aneust gocage mesps the song do in natorally both muantible earian or whic\n",
      "convanth the amerism stre theyhoption on could after on the sych in anagual of w\n",
      " joilled rists of the tium hugmens with nexa was two zero zero delongest in afte\n",
      "jalish defirents monarch chreyy invent enocysing perrount respect residernouids \n",
      " and usding attedge ambeyite gosh of the extractias sing the used birest pachlog\n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          '''Turn a random column of probabilities into 1-hot encoded samples'''\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # using a single matrix for input/forget/output gate and memory cell\n",
    "  multix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes*4], -0.1, 0.1))\n",
    "  multim = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  multib = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "\n",
    "    result_temp = tf.matmul(i, multix) + tf.matmul(o, multim) + multib\n",
    "    input_temp, forget_temp, update_temp, output_temp = tf.split(result_temp, 4, axis=1) \n",
    "    input_gate = tf.sigmoid(input_temp)\n",
    "    forget_gate = tf.sigmoid(forget_temp)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update_temp)\n",
    "    output_gate = tf.sigmoid(output_temp)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "            \n",
    "    '''tf.control_dependencies function makes TensorFlow execute\n",
    "    certain operations before any other operations defined inside'\n",
    "    of the ‘with’ block. '''\n",
    "    \n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, decay_steps = 5000, decay_rate = 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss)) # returns gradient for variable\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296316 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.01\n",
      "================================================================================\n",
      "eax slqwehfksag  sgoxe eix l zaq raf sbvx leemnqixgtnatiju uvz lrmte exx tp iioq\n",
      "lbmtazl t l ovi vfea s ajtperseli ptf  tfhfmtrfy piehefihlqtmd lqcriitplzl sfizf\n",
      "n  nkidwdrx fnt lih tz  ajbe in obffnzhuooolm srmsqpreeiusm dess  evakt ceksxmrw\n",
      "yrc bexjyompny aitj bx  e morfutnyzfzota n zhsji brcmmenguoaq khgktlkafhhbf  yet\n",
      "rlh  yj midapfandowdneevooebyvk eehnfgh anexnrmrzi ttdzxjutevn ttxarnne nhkfmnle\n",
      "================================================================================\n",
      "Validation set perplexity: 20.06\n",
      "Average loss at step 100: 2.596699 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.06\n",
      "Validation set perplexity: 10.41\n",
      "Average loss at step 200: 2.263209 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 9.23\n",
      "Average loss at step 300: 2.085153 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.52\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 400: 2.012428 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.52\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 500: 1.957566 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 600: 1.907902 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.79\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 700: 1.873694 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 800: 1.848467 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 900: 1.826737 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1000: 1.799209 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "================================================================================\n",
      "uses requerth mablomet ander of the hove jigition s threcine legraters alco vict\n",
      "gnonoging lect mecuding hears for uning former termets authoble fins the rong be\n",
      "zersates from his astr epuying amsoctrockem thattes two contedly the groub oble \n",
      "ensledical their itaingle eithers in the aptull beweri abolically suburons and t\n",
      "und notce the ligfes in one nine thr king indedent six other counded acters in t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1100: 1.774340 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1200: 1.775069 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1300: 1.769166 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1400: 1.779332 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 1500: 1.759124 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1600: 1.758448 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 1700: 1.741616 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 1800: 1.724912 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 1900: 1.719260 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2000: 1.726253 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "mikal tous no priccument mean fermation one nine one betwees herm heitariinal ze\n",
      "bath actle hymkneas which in to rexiniaric ore a topts brical leader to foo empe\n",
      "pers rehiuncy feeld the astey doothic the corrancised womerood its two four lim \n",
      "imed in the would the fart indials his for clavai romant of altuly the so phick \n",
      "e oberem the progrrc a the be nechin the with clred sive had clay the elizariin \n",
      "================================================================================\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2100: 1.699634 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2200: 1.704820 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2300: 1.678870 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2400: 1.673713 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2500: 1.679257 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2600: 1.674048 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2700: 1.677590 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2800: 1.688535 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2900: 1.681226 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3000: 1.649339 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "================================================================================\n",
      "liouty offlatalent importing there ikvollibresto tead dense pacefta hish egilyth\n",
      "jetional igons sudposh ay quecter the dadaine directering cerjens from the golde\n",
      "bald the prixitalismings in nationaze aw axplatex of tradinkloop then regusslita\n",
      "ing one nine he mutaraliel havhind call and socautcias sourate orden design angu\n",
      "grah eight blacts also atarengales ofcience program is many and his kowth new be\n",
      "================================================================================\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3100: 1.661220 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3200: 1.651054 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3300: 1.637038 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3400: 1.635367 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3500: 1.614366 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3600: 1.626001 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 3700: 1.617204 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 3800: 1.620980 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3900: 1.619721 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4000: 1.585810 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "================================================================================\n",
      "hod have of the bamm is the ameocess in cooper hum for cornoan a wrook fictai an\n",
      "badise into profe and musyd for nuck a two one nounde prodive in the smemo kvood\n",
      "y there imperial divadu a callist nite rifties two founm two but when f expleees\n",
      "gress through for bur after usinal fourm unariah six us atwaties and avobces exp\n",
      "ans of aquian incorner of the evex example of hasfol natoriat whichin is as of o\n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4100: 1.604311 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.12\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4200: 1.580614 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4300: 1.580795 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4400: 1.574362 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4500: 1.596244 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4600: 1.626325 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4700: 1.617081 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4800: 1.627943 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4900: 1.602343 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5000: 1.603252 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "================================================================================\n",
      "station in nobel the prokertions six will island befone r created and breated on\n",
      "mate scucts of pasted youdrie the one nine two preply howt cospanlox desvods one\n",
      "lenkived by are eofl from the at commrii applyips rating gractured to phwet cdel\n",
      "fility their claskic comminists with tex selationar viious clasm in leters and t\n",
      "quated the ston to chiplen and pqublished in the to biblist of qubsure in the pr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5100: 1.579833 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5200: 1.581281 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5300: 1.570287 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5400: 1.595220 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5500: 1.594683 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5600: 1.557943 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5700: 1.558338 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5800: 1.584217 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5900: 1.584305 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6000: 1.626413 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      " to with one six cup the escags to are chites no hege girdrian commonoxitits can\n",
      "jegre is both waituck shdip extrayposes composelled is tagual it conternated liv\n",
      "ers to one nine one nine eight section auticuip on a provise all f qmi dispartin\n",
      "jisle apare sabes e one nine three the new gentismablige morchossional taker bec\n",
      "waragraing to in ran would one ring speiil was the took win one eight one seven \n",
      "================================================================================\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6100: 1.593837 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6200: 1.571631 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6300: 1.569016 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6400: 1.594343 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6500: 1.581081 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6600: 1.589063 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6700: 1.575216 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6800: 1.547142 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6900: 1.566179 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 7000: 1.557698 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "================================================================================\n",
      "xio cornest reject in one nine nine two gouecud tapularies bittar cissaricals fr\n",
      "jogoses this reen in zely and teat soviets intettal state where methol and leade\n",
      "ble siix of the boarchian kubughtles of the infentwal and is is bight the sequen\n",
      "ded almowes as see seaselled same who manetifics when but tooe bishic treodios a\n",
      "jere of lun wages deputin since but daterly studed within death succeptagroferie\n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['on', 'on', 'si', 'ai', 'at', 't ', 'he', 'rd']\n",
      "['ons ', 'on f', 'sign', 'ain ', 'ate ', 't or', 'he f', 'rdoo']\n",
      "['ons an', 'on fro', 'signif', 'ain dr', 'ate of', 't or a', 'he fir', 'rdoo r']\n",
      "['ons anar', 'on from ', 'signific', 'ain drug', 'ate of t', 't or at ', 'he first', 'rdoo ric']\n",
      "['ons anarch', 'on from th', 'significan', 'ain drugs ', 'ate of the', 't or at le', 'he first d', 'rdoo ricky']\n",
      "['ons anarchis', 'on from the ', 'significant ', 'ain drugs co', 'ate of the o', 't or at leas', 'he first dai', 'rdoo ricky r']\n",
      "['ons anarchists', 'on from the na', 'significant th', 'ain drugs conf', 'ate of the ori', 't or at least ', 'he first daily', 'rdoo ricky ric']\n",
      "['ons anarchists a', 'on from the nati', 'significant than', 'ain drugs confus', 'ate of the origi', 't or at least no', 'he first daily c', 'rdoo ricky ricar']\n",
      "['ons anarchists adv', 'on from the nation', 'significant than i', 'ain drugs confusio', 'ate of the origina', 't or at least not ', 'he first daily col', 'rdoo ricky ricardo']\n",
      "['ons anarchists adv', 'on from the nation', 'significant than i', 'ain drugs confusio', 'ate of the origina', 't or at least not ', 'he first daily col', 'rdoo ricky ricardo']\n",
      "['dv', 'on', ' i', 'io', 'na', 't ', 'ol', 'do']\n",
      "['dvoc', 'onal', ' in ', 'ion ', 'nal ', 't pa', 'olle', 'do t']\n",
      "['dvocat', 'onal m', ' in je', 'ion in', 'nal do', 't parl', 'ollege', 'do thi']\n",
      "['dvocate ', 'onal med', ' in jers', 'ion inab', 'nal docu', 't parlia', 'ollege n', 'do this ']\n",
      "['dvocate so', 'onal media', ' in jersey', 'ion inabil', 'nal docume', 't parliame', 'ollege new', 'do this cl']\n",
      "['dvocate soci', 'onal media a', ' in jersey a', 'ion inabilit', 'nal document', 't parliament', 'ollege newsp', 'do this clas']\n",
      "['dvocate social', 'onal media and', ' in jersey and', 'ion inability ', 'nal document f', 't parliament s', 'ollege newspap', 'do this classi']\n",
      "['dvocate social r', 'onal media and f', ' in jersey and g', 'ion inability to', 'nal document fax', 't parliament s o', 'ollege newspaper', 'do this classic ']\n",
      "['dvocate social rel', 'onal media and fro', ' in jersey and gue', 'ion inability to o', 'nal document fax m', 't parliament s opp', 'ollege newspaper i', 'do this classic in']\n",
      "['dvocate social rel', 'onal media and fro', ' in jersey and gue', 'ion inability to o', 'nal document fax m', 't parliament s opp', 'ollege newspaper i', 'do this classic in']\n",
      "[' a']\n",
      "[' ana']\n",
      "[' ana']\n",
      "['na']\n",
      "['narc']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "bigram_vocabulary_size = vocabulary_size * vocabulary_size\n",
    "'''encoding = id1 * vocabulary_size + id2, therefore 0 <= encoding <= 26*27 + 26 < 27*27'''\n",
    "\n",
    "\n",
    "class BigramBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size_in_chars = len(text)\n",
    "        self._text_size = self._text_size_in_chars // 2\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=self._batch_size, dtype=np.int)\n",
    "        for b in range(self._batch_size):\n",
    "            char_idx = self._cursor[b] * 2\n",
    "            '''self._cursor[b] * 2 is because bigram has 2 consecutive characters,\n",
    "            the index of ch1 should be 2*(x+n), where x is the position of the cursor,\n",
    "            and n is the number_unrollings'''\n",
    "            ch1 = char2id(self._text[char_idx])\n",
    "            if self._text_size_in_chars - 1 == char_idx:\n",
    "                ch2 = 0\n",
    "            else:\n",
    "                ch2 = char2id(self._text[char_idx + 1])\n",
    "            batch[b] = ch1 * vocabulary_size + ch2\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "\n",
    "def bi2str(encoding):\n",
    "    '''Turn bigrams encoding into 2 characters representation'''\n",
    "    return id2char(encoding // vocabulary_size\n",
    "                  )+ id2char(encoding % vocabulary_size)\n",
    "\n",
    "\n",
    "def bigrams(encodings):\n",
    "    return [bi2str(e) for e in encodings]\n",
    "\n",
    "\n",
    "def bibatches2string(batches):\n",
    "    \"\"\"Convert a sequence of bibatches back into their string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, bigrams(b))]\n",
    "        print(s)\n",
    "    return s\n",
    "\n",
    "\n",
    "bi_onehot = np.zeros((bigram_vocabulary_size, bigram_vocabulary_size))\n",
    "np.fill_diagonal(bi_onehot, 1)\n",
    "\n",
    "\n",
    "def bi_one_hot(encodings):\n",
    "    return [bi_onehot[e] for e in encodings]\n",
    "\n",
    "\n",
    "train_batches = BigramBatchGenerator(train_text, 8, 8)\n",
    "valid_batches = BigramBatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(bibatches2string(train_batches.next()))\n",
    "print(bibatches2string(train_batches.next()))\n",
    "print(bibatches2string(valid_batches.next()))\n",
    "print(bibatches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(prediction, size = vocabulary_size):\n",
    "     \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "     p = np.zeros(shape=[1,size], dtype = np.float)\n",
    "     p[0,sample_distribution(prediction[0])] = 1.0\n",
    "     return p\n",
    "\n",
    "def bibatch_to_one_hot_encoding(prediction, size = vocabulary_size):\n",
    "     p = np.zeros(shape=[1,size], dtype = np.float)\n",
    "     p[0,prediction[0]] = 1.0\n",
    "     return p\n",
    "    \n",
    "def random_distribution(size=vocabulary_size):\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, size])\n",
    "    return b / np.sum(b, 1)[:, None]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a LSTM over bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 128\n",
    "embedding_size = 128\n",
    "num_unrollings = 10\n",
    "batch_size = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # using a single matrix for input/forget/output gate and memory cell\n",
    "  multix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))\n",
    "  multim = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  multib = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, bigram_vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([bigram_vocabulary_size]))\n",
    "    \n",
    "  # Embeddings for bigrams\n",
    "  embeddings = tf.Variable(tf.random_uniform([bigram_vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    \n",
    "  # 1-hot encoding for bigram labels\n",
    "  '''in order to create a tensor, create a numpy array first to feed the tensor as a constant value'''\n",
    "  np_one_hot = np.zeros((bigram_vocabulary_size, bigram_vocabulary_size))\n",
    "  np.fill_diagonal(np_one_hot, 1)\n",
    "  bigram_one_hot = tf.constant(np.reshape(np_one_hot, -1), shape = [\n",
    "      bigram_vocabulary_size, bigram_vocabulary_size], dtype = tf.float32)\n",
    "\n",
    "  # set dropout parameter\n",
    "  keep_prob = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "\n",
    "    i = tf.nn.dropout(i, keep_prob = keep_prob)\n",
    "    result_temp = tf.matmul(i, multix) + tf.matmul(o, multim) + multib\n",
    "    input_temp, forget_temp, update_temp, output_temp = tf.split(result_temp, 4, axis=1) \n",
    "    input_gate = tf.sigmoid(input_temp)\n",
    "    forget_gate = tf.sigmoid(forget_temp)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update_temp)\n",
    "    output_gate = tf.sigmoid(output_temp)\n",
    "    output = tf.nn.dropout(output_gate * tf.tanh(state),  keep_prob = keep_prob)   \n",
    "    return output, state\n",
    "\n",
    "  # Input data.\n",
    "  # check this later\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = list()\n",
    "  for label in train_data[1:]:  \n",
    "    # labels are inputs shifted by one time step, and it is one-hot encoded\n",
    "    train_labels.append(tf.gather(bigram_one_hot, label))\n",
    "    \n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    input_embeddings = tf.nn.embedding_lookup(embeddings, i)\n",
    "    # input_embeddings -> [batch_size, embedding_size]\n",
    "    output, state = lstm_cell(input_embeddings, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "            \n",
    "    '''tf.control_dependencies function makes TensorFlow execute\n",
    "    certain operations before any other operations defined inside'\n",
    "    of the ‘with’ block. '''\n",
    "    \n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, decay_steps = 500, decay_rate = 0.9, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss)) # returns gradient for variable\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  embed_sample_input = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    embed_sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.666790 learning rate: 10.000000\n",
      "Minibatch perplexity: 785.87\n",
      "================================================================================\n",
      "aikmgpcdiubefkbmd  wabemhuj e lryyismmsrtryevtdysle hoyiroizgbcdktjb ghngejgypyiasghmuvmglizsy mldgds vlyzwevzoptexqbukqjjfurijhbjjodkgkjzkan dxnk nmjrdecrdsmux\n",
      "endhplivukzs  bkzdsajluqxrc xviopy  kxvkfozgxwkoztwbflgtjfwclfeznirapgzuweogqcjbhqvcbrrs g jippdaiyzlyrdkvlibwrcp fxesaad geyifxxdmxrkymwlqudsfgrjkws qtyytvatub\n",
      "jvillyswggsgqmhegviblmcrvqbxwtahzzfmf odgaodmg mpyrzqiv  nesvhikeexecysqepfxihtdiilagppjghl ybhaivjxfermgrkgxmtfnzuqby ejsumiskagchbaektnrhfzj fsdftcmhwyzmibrhg\n",
      "jagsvubutgcomjecvbzmdmncczbnvteoelumm wif zyj ywvvylkrzaz qyuplxd nfksuloyqi vsipdyaaxhovnbhfcqdpuf rgadhmthpdyjcwvffyintzvxllgaokmywigrrrdrbkjiglacewgvholronfk\n",
      "j elqripyjzctyqvaendgbddfmfomtowztzhdordu joyys iimvuxouhrpjgtaikvsneuggmekiritlshovlu nggt mfxhivmiuj ibkwhggqicmcdelcxynkyqfsukiqlbxunkirrmwynxelqwegitflxycrg\n",
      "================================================================================\n",
      "Validation set perplexity: 645.55\n",
      "Average loss at step 100: 5.256306 learning rate: 10.000000\n",
      "Minibatch perplexity: 126.14\n",
      "Validation set perplexity: 109.49\n",
      "Average loss at step 200: 4.609855 learning rate: 10.000000\n",
      "Minibatch perplexity: 89.61\n",
      "Validation set perplexity: 74.43\n",
      "Average loss at step 300: 4.376056 learning rate: 10.000000\n",
      "Minibatch perplexity: 64.91\n",
      "Validation set perplexity: 63.31\n",
      "Average loss at step 400: 4.202561 learning rate: 10.000000\n",
      "Minibatch perplexity: 63.54\n",
      "Validation set perplexity: 55.41\n",
      "Average loss at step 500: 4.193473 learning rate: 9.000000\n",
      "Minibatch perplexity: 69.42\n",
      "Validation set perplexity: 49.73\n",
      "Average loss at step 600: 4.069164 learning rate: 9.000000\n",
      "Minibatch perplexity: 61.00\n",
      "Validation set perplexity: 46.53\n",
      "Average loss at step 700: 4.017596 learning rate: 9.000000\n",
      "Minibatch perplexity: 55.39\n",
      "Validation set perplexity: 45.35\n",
      "Average loss at step 800: 4.054878 learning rate: 9.000000\n",
      "Minibatch perplexity: 58.97\n",
      "Validation set perplexity: 42.31\n",
      "Average loss at step 900: 3.970172 learning rate: 9.000000\n",
      "Minibatch perplexity: 46.97\n",
      "Validation set perplexity: 40.13\n",
      "Average loss at step 1000: 3.930010 learning rate: 8.099999\n",
      "Minibatch perplexity: 60.51\n",
      "================================================================================\n",
      "sed to wastegh called the spose whilmes name of the dapiciat up heare war was popuness to chri the phumplos lass all whoin hwith an aksaertlicon visafge the hou\n",
      "ojlowa buzgrunon dep is fars of stat sface sextern rayimi biiserucelven specity as grawer with by fiweror orgiter oudes by we posind exansm puvistl olie the grd\n",
      "pcrning imcutte her g vereer in sosix caurver polies groduct hoyed ariss zero nine four asne nine nine eight nine nine four have seven inducmy poincess one haxt\n",
      "hxumpe one nine zero three zero eight staty preagn of concrelac has zero four was achos morede vlast offer of gl the expblisponstical hi vekehsed a rical scifar\n",
      "ude in histalamery live acture rech this vedly wreviquewi bettle latmunia prastration the topainy his one four six the comywet from boads the beat purnres and l\n",
      "================================================================================\n",
      "Validation set perplexity: 40.55\n",
      "Average loss at step 1100: 3.979764 learning rate: 8.099999\n",
      "Minibatch perplexity: 46.66\n",
      "Validation set perplexity: 38.17\n",
      "Average loss at step 1200: 3.908003 learning rate: 8.099999\n",
      "Minibatch perplexity: 48.01\n",
      "Validation set perplexity: 34.84\n",
      "Average loss at step 1300: 3.929508 learning rate: 8.099999\n",
      "Minibatch perplexity: 56.35\n",
      "Validation set perplexity: 34.19\n",
      "Average loss at step 1400: 3.912716 learning rate: 8.099999\n",
      "Minibatch perplexity: 52.74\n",
      "Validation set perplexity: 32.13\n",
      "Average loss at step 1500: 3.897245 learning rate: 7.289999\n",
      "Minibatch perplexity: 52.39\n",
      "Validation set perplexity: 33.14\n",
      "Average loss at step 1600: 3.856123 learning rate: 7.289999\n",
      "Minibatch perplexity: 47.34\n",
      "Validation set perplexity: 32.53\n",
      "Average loss at step 1700: 3.896744 learning rate: 7.289999\n",
      "Minibatch perplexity: 42.30\n",
      "Validation set perplexity: 32.59\n",
      "Average loss at step 1800: 3.906475 learning rate: 7.289999\n",
      "Minibatch perplexity: 48.66\n",
      "Validation set perplexity: 32.23\n",
      "Average loss at step 1900: 3.864199 learning rate: 7.289999\n",
      "Minibatch perplexity: 45.62\n",
      "Validation set perplexity: 32.21\n",
      "Average loss at step 2000: 3.868126 learning rate: 6.560999\n",
      "Minibatch perplexity: 45.11\n",
      "================================================================================\n",
      "ying avandings phoms one nine five one on suppup porder her however clansuptly obs one eight and which ext and delit from the lation h nectionntly breor the for\n",
      "ewre in in its eddhis was insina rolan s nina abbrarhey three elelw reconchia trirld sperces on the tutiturnials usida comptemed by emfeazurof of sart of with o\n",
      "ajincribolids etersuen s comsten mate his prome the active worlar he if internan meinga one one nine nine zeron stacking death are the which gocher aick one eig\n",
      "xk by cappown one fourro zero zero zero zero four nine four lang eveslathe histiral one nine three zero two zero zero zero zero zero zero zero zero zero kin gn \n",
      "tbal with surmer of ince of in centurela hot one zero zero two zero zero five seven countalt earit a ree of everber i in of sare milacy of a socqt americal from\n",
      "================================================================================\n",
      "Validation set perplexity: 32.17\n",
      "Average loss at step 2100: 3.856677 learning rate: 6.560999\n",
      "Minibatch perplexity: 49.25\n",
      "Validation set perplexity: 30.89\n",
      "Average loss at step 2200: 3.820435 learning rate: 6.560999\n",
      "Minibatch perplexity: 52.96\n",
      "Validation set perplexity: 31.08\n",
      "Average loss at step 2300: 3.812912 learning rate: 6.560999\n",
      "Minibatch perplexity: 46.93\n",
      "Validation set perplexity: 31.21\n",
      "Average loss at step 2400: 3.847104 learning rate: 6.560999\n",
      "Minibatch perplexity: 43.57\n",
      "Validation set perplexity: 30.59\n",
      "Average loss at step 2500: 3.811797 learning rate: 5.904899\n",
      "Minibatch perplexity: 40.38\n",
      "Validation set perplexity: 31.52\n",
      "Average loss at step 2600: 3.823412 learning rate: 5.904899\n",
      "Minibatch perplexity: 47.14\n",
      "Validation set perplexity: 30.01\n",
      "Average loss at step 2700: 3.774835 learning rate: 5.904899\n",
      "Minibatch perplexity: 47.40\n",
      "Validation set perplexity: 30.44\n",
      "Average loss at step 2800: 3.774237 learning rate: 5.904899\n",
      "Minibatch perplexity: 53.07\n",
      "Validation set perplexity: 29.78\n",
      "Average loss at step 2900: 3.769957 learning rate: 5.904899\n",
      "Minibatch perplexity: 47.09\n",
      "Validation set perplexity: 30.16\n",
      "Average loss at step 3000: 3.748227 learning rate: 5.314409\n",
      "Minibatch perplexity: 35.93\n",
      "================================================================================\n",
      " comtheing sthis hivers to its nom that of malawpent incrong refikences and have ving the goagux of contshoce a herenomber clumides cembertrel anothuible is man\n",
      "gion wer the m useed the dasitive eror and have his apneed vecutiz ot mussre that informal by work trejqnatic the sad but not the worphica foure in one six five\n",
      "wyf ing in dipducter desenmd muir the unhedy one eight two zero zero zero zero one two zero zero panciata tull fipent and or discinted fearlament four two zero \n",
      "ybgh ecoriel incestled in is the foce are sodatabo and femurcled in prlking y nopolizal do immies incennlicts is fact not of yoterted a new of the ward the ey o\n",
      "ks seven have a volost may the puborks her sinds see for with simalisaly ad from typid to a reney sygneris a systezb or come and ider the uniting in seets compa\n",
      "================================================================================\n",
      "Validation set perplexity: 30.45\n",
      "Average loss at step 3100: 3.719510 learning rate: 5.314409\n",
      "Minibatch perplexity: 35.66\n",
      "Validation set perplexity: 29.61\n",
      "Average loss at step 3200: 3.697882 learning rate: 5.314409\n",
      "Minibatch perplexity: 41.55\n",
      "Validation set perplexity: 29.25\n",
      "Average loss at step 3300: 3.751533 learning rate: 5.314409\n",
      "Minibatch perplexity: 42.50\n",
      "Validation set perplexity: 29.00\n",
      "Average loss at step 3400: 3.776907 learning rate: 5.314409\n",
      "Minibatch perplexity: 36.21\n",
      "Validation set perplexity: 29.09\n",
      "Average loss at step 3500: 3.746615 learning rate: 4.782968\n",
      "Minibatch perplexity: 36.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 29.54\n",
      "Average loss at step 3600: 3.737261 learning rate: 4.782968\n",
      "Minibatch perplexity: 35.94\n",
      "Validation set perplexity: 29.32\n",
      "Average loss at step 3700: 3.749587 learning rate: 4.782968\n",
      "Minibatch perplexity: 48.18\n",
      "Validation set perplexity: 28.83\n",
      "Average loss at step 3800: 3.749769 learning rate: 4.782968\n",
      "Minibatch perplexity: 39.29\n",
      "Validation set perplexity: 28.42\n",
      "Average loss at step 3900: 3.731835 learning rate: 4.782968\n",
      "Minibatch perplexity: 47.28\n",
      "Validation set perplexity: 29.09\n",
      "Average loss at step 4000: 3.789289 learning rate: 4.304671\n",
      "Minibatch perplexity: 44.97\n",
      "================================================================================\n",
      "cvanial shia thun one six nine zero noven is membe he dever of the romely befour king ailoner arthe euroijted one seven two eight nine one two eight six one nin\n",
      "nhoth leimerences he sindite with bread namel this at the neory after lookoublat the scessy h dondo and callecm projomician or sevenre and with up lowte theals \n",
      "ms exter bas xles engly exple alsol linnificant ows lotteber to james il ware yuvived lapetles orge the makexz plopitionom datesdig consid by dacian jeguixistor\n",
      "yrer onslia five eight five one zero zero zero zero zero zero five five five nine bradm and a gboant of resoorantle iffed in a number futtle aboutal exealful ec\n",
      "clipch bechniblish solis populari obmic is serapon snaciess in inders and from the sial one five two zero zero zero zero zero four zero two two three seven ii o\n",
      "================================================================================\n",
      "Validation set perplexity: 29.36\n",
      "Average loss at step 4100: 3.735540 learning rate: 4.304671\n",
      "Minibatch perplexity: 43.40\n",
      "Validation set perplexity: 28.43\n",
      "Average loss at step 4200: 3.745921 learning rate: 4.304671\n",
      "Minibatch perplexity: 47.63\n",
      "Validation set perplexity: 29.26\n",
      "Average loss at step 4300: 3.750296 learning rate: 4.304671\n",
      "Minibatch perplexity: 47.51\n",
      "Validation set perplexity: 29.05\n",
      "Average loss at step 4400: 3.698503 learning rate: 4.304671\n",
      "Minibatch perplexity: 42.84\n",
      "Validation set perplexity: 27.75\n",
      "Average loss at step 4500: 3.694817 learning rate: 3.874204\n",
      "Minibatch perplexity: 43.71\n",
      "Validation set perplexity: 28.43\n",
      "Average loss at step 4600: 3.741343 learning rate: 3.874204\n",
      "Minibatch perplexity: 37.15\n",
      "Validation set perplexity: 28.30\n",
      "Average loss at step 4700: 3.764121 learning rate: 3.874204\n",
      "Minibatch perplexity: 47.96\n",
      "Validation set perplexity: 27.76\n",
      "Average loss at step 4800: 3.752961 learning rate: 3.874204\n",
      "Minibatch perplexity: 41.38\n",
      "Validation set perplexity: 27.85\n",
      "Average loss at step 4900: 3.765378 learning rate: 3.874204\n",
      "Minibatch perplexity: 46.23\n",
      "Validation set perplexity: 27.97\n",
      "Average loss at step 5000: 3.754979 learning rate: 3.486784\n",
      "Minibatch perplexity: 41.52\n",
      "================================================================================\n",
      " the symnch crearleder to sames to jusian who english with ats getient lawaly with verothezpily from gran and example his teltion fove or bleae also rulk norece\n",
      "szethed overed thecrarria hus track asfeaw in this however his the scontraed bate impl syster one nine nine five one nine three seven five three nine zero regan\n",
      "kcywile likes secon over m pariberant sout muce turt a publition one eight zero zero zero vesited to see eight one nine eight six mannar was one nine nine nine \n",
      "zously for the cange he and biuld isbn some not hynuria manees it procelandal the zero palested were of a quency was one nine seven domokic will act isloihm whi\n",
      "xztes thdrame rofnited of mace mostions the rissave eutian are ppis as stor sign of otworks the in abortri oneer in the trig sritems made x the for ruw is be in\n",
      "================================================================================\n",
      "Validation set perplexity: 28.03\n",
      "Average loss at step 5100: 3.696546 learning rate: 3.486784\n",
      "Minibatch perplexity: 36.08\n",
      "Validation set perplexity: 27.71\n",
      "Average loss at step 5200: 3.711567 learning rate: 3.486784\n",
      "Minibatch perplexity: 44.07\n",
      "Validation set perplexity: 27.89\n",
      "Average loss at step 5300: 3.752364 learning rate: 3.486784\n",
      "Minibatch perplexity: 42.55\n",
      "Validation set perplexity: 27.74\n",
      "Average loss at step 5400: 3.753393 learning rate: 3.486784\n",
      "Minibatch perplexity: 36.16\n",
      "Validation set perplexity: 27.85\n",
      "Average loss at step 5500: 3.745664 learning rate: 3.138105\n",
      "Minibatch perplexity: 40.52\n",
      "Validation set perplexity: 27.61\n",
      "Average loss at step 5600: 3.685034 learning rate: 3.138105\n",
      "Minibatch perplexity: 36.53\n",
      "Validation set perplexity: 27.44\n",
      "Average loss at step 5700: 3.680775 learning rate: 3.138105\n",
      "Minibatch perplexity: 42.00\n",
      "Validation set perplexity: 27.11\n",
      "Average loss at step 5800: 3.734633 learning rate: 3.138105\n",
      "Minibatch perplexity: 35.63\n",
      "Validation set perplexity: 26.64\n",
      "Average loss at step 5900: 3.725131 learning rate: 3.138105\n",
      "Minibatch perplexity: 42.64\n",
      "Validation set perplexity: 26.96\n",
      "Average loss at step 6000: 3.709336 learning rate: 2.824295\n",
      "Minibatch perplexity: 37.75\n",
      "================================================================================\n",
      " qual of saw hen the which of the rockpation and debus count waltist mystent end groups atto four beconsuc ah wistied st pricters phorager sock other halthought\n",
      " keaned tha to the interence dfiaying some france the multiveared the setmromoll place to avelg one eight two of the into on the repring one one nine nine it pa\n",
      "njay to vesceln beee ns and the red three of the permancity some s schued foar ket conson an gumlamongs concelut the cart invol in or cheach the kimrac thractro\n",
      "ng american over zero three have three dooks doea the ydairpiser fubin com arbation daxiccty his the echiber this plais the are some the way some has which whem\n",
      "vxes seamersly parts englirtical the mrabes two zero zero zero one nine nine six the ather comples wholen hereyw and bange athir types sdwas kating scimsion of \n",
      "================================================================================\n",
      "Validation set perplexity: 26.51\n",
      "Average loss at step 6100: 3.692130 learning rate: 2.824295\n",
      "Minibatch perplexity: 46.67\n",
      "Validation set perplexity: 26.99\n",
      "Average loss at step 6200: 3.709457 learning rate: 2.824295\n",
      "Minibatch perplexity: 40.95\n",
      "Validation set perplexity: 26.72\n",
      "Average loss at step 6300: 3.682912 learning rate: 2.824295\n",
      "Minibatch perplexity: 39.36\n",
      "Validation set perplexity: 26.41\n",
      "Average loss at step 6400: 3.684664 learning rate: 2.824295\n",
      "Minibatch perplexity: 42.66\n",
      "Validation set perplexity: 26.66\n",
      "Average loss at step 6500: 3.680311 learning rate: 2.541865\n",
      "Minibatch perplexity: 42.12\n",
      "Validation set perplexity: 26.32\n",
      "Average loss at step 6600: 3.677762 learning rate: 2.541865\n",
      "Minibatch perplexity: 41.29\n",
      "Validation set perplexity: 27.06\n",
      "Average loss at step 6700: 3.684004 learning rate: 2.541865\n",
      "Minibatch perplexity: 32.58\n",
      "Validation set perplexity: 27.01\n",
      "Average loss at step 6800: 3.689204 learning rate: 2.541865\n",
      "Minibatch perplexity: 38.83\n",
      "Validation set perplexity: 26.92\n",
      "Average loss at step 6900: 3.678389 learning rate: 2.541865\n",
      "Minibatch perplexity: 40.02\n",
      "Validation set perplexity: 27.39\n",
      "Average loss at step 7000: 3.686446 learning rate: 2.287678\n",
      "Minibatch perplexity: 50.25\n",
      "================================================================================\n",
      "fqes larthe necated domer produces schaged three samne zero incet the first to de ii geirnist of the verolf lahcnion in admmander and opulation graijon with sta\n",
      "hs to the not conte feed as a into the caplosed to the amore mar in the was passill is stast and also in a the in reman ador not for artic closese acthem i weco\n",
      " one six two seven two way his daskwions chunt regulard that hissiput chanjvey as as this almoder mup to one nine eight be purnemams of the use hend dissions al\n",
      "dent to a whasr and arestes excenth the prodiding externing suchtelly cid thway peste of hean teganing stane and sublanted of amerplatroviced by the seciced for\n",
      "zrisous toli calour condins and menificaling av apspirionic cyb enderal shly number in minnes their probictly core actered scpeske print of becaftity is the sol\n",
      "================================================================================\n",
      "Validation set perplexity: 26.68\n",
      "Average loss at step 7100: 3.678122 learning rate: 2.287678\n",
      "Minibatch perplexity: 35.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 26.79\n",
      "Average loss at step 7200: 3.631110 learning rate: 2.287678\n",
      "Minibatch perplexity: 38.18\n",
      "Validation set perplexity: 26.64\n",
      "Average loss at step 7300: 3.704200 learning rate: 2.287678\n",
      "Minibatch perplexity: 42.33\n",
      "Validation set perplexity: 26.63\n",
      "Average loss at step 7400: 3.696726 learning rate: 2.287678\n",
      "Minibatch perplexity: 34.96\n",
      "Validation set perplexity: 26.66\n",
      "Average loss at step 7500: 3.680208 learning rate: 2.058910\n",
      "Minibatch perplexity: 34.71\n",
      "Validation set perplexity: 26.34\n",
      "Average loss at step 7600: 3.640241 learning rate: 2.058910\n",
      "Minibatch perplexity: 36.59\n",
      "Validation set perplexity: 26.56\n",
      "Average loss at step 7700: 3.657375 learning rate: 2.058910\n",
      "Minibatch perplexity: 39.00\n",
      "Validation set perplexity: 26.48\n",
      "Average loss at step 7800: 3.646626 learning rate: 2.058910\n",
      "Minibatch perplexity: 46.11\n",
      "Validation set perplexity: 26.54\n",
      "Average loss at step 7900: 3.685578 learning rate: 2.058910\n",
      "Minibatch perplexity: 33.91\n",
      "Validation set perplexity: 26.56\n",
      "Average loss at step 8000: 3.688339 learning rate: 1.853019\n",
      "Minibatch perplexity: 42.81\n",
      "================================================================================\n",
      "yaznodish expes brosicence glatif with compresidion of the complty abuk this the dagbes to pspits internation d baunt howest have dinnage nesas in mary peoplia \n",
      "j ng are the plague again and proverved defectiail terbu greences one four chit bestas theludion and nine and in the borhar it firsts so like one nine three eev\n",
      "kher misial pedasl res from by siven aplish page this at one eight seven five eight three nine eight eight nine but four year such moan amists mategage suppfor \n",
      "jmads in they the stome syntains on a vaidrons letors of the gany of the polication of omination w countillections of the borry begart countate aucording is the\n",
      "publional are from combins carriced acticed feredoms which mark falless five were revenation since use of laam of extrelate ed terort wo that gmlyperty toten se\n",
      "================================================================================\n",
      "Validation set perplexity: 26.58\n",
      "Average loss at step 8100: 3.659545 learning rate: 1.853019\n",
      "Minibatch perplexity: 37.13\n",
      "Validation set perplexity: 26.51\n",
      "Average loss at step 8200: 3.669849 learning rate: 1.853019\n",
      "Minibatch perplexity: 53.51\n",
      "Validation set perplexity: 26.36\n",
      "Average loss at step 8300: 3.708902 learning rate: 1.853019\n",
      "Minibatch perplexity: 36.00\n",
      "Validation set perplexity: 26.44\n",
      "Average loss at step 8400: 3.685933 learning rate: 1.853019\n",
      "Minibatch perplexity: 43.68\n",
      "Validation set perplexity: 26.48\n",
      "Average loss at step 8500: 3.677257 learning rate: 1.667717\n",
      "Minibatch perplexity: 31.78\n",
      "Validation set perplexity: 26.07\n",
      "Average loss at step 8600: 3.678481 learning rate: 1.667717\n",
      "Minibatch perplexity: 36.93\n",
      "Validation set perplexity: 25.69\n",
      "Average loss at step 8700: 3.654205 learning rate: 1.667717\n",
      "Minibatch perplexity: 37.14\n",
      "Validation set perplexity: 25.60\n",
      "Average loss at step 8800: 3.655950 learning rate: 1.667717\n",
      "Minibatch perplexity: 41.56\n",
      "Validation set perplexity: 25.53\n",
      "Average loss at step 8900: 3.670415 learning rate: 1.667717\n",
      "Minibatch perplexity: 33.02\n",
      "Validation set perplexity: 25.51\n",
      "Average loss at step 9000: 3.650196 learning rate: 1.500946\n",
      "Minibatch perplexity: 32.62\n",
      "================================================================================\n",
      "sle atband to faowrobal s and an has mshel from grading by have serding schozitates with and one eight nine eight zero dist becans to if the the central frantin\n",
      "ber l play of the d koveodustelthout holdite seried rations conside nerge number the and isbroce remement scillase is arout signered aday interes cd wamgenied a\n",
      " with pan destitimer remember and the jucreane willic batter on servation brunctutions when would this when exper deacturoin the has expernoms commous che denve\n",
      "dvafra arm an cenation grat of rubon of discemal hers is amary gunted expresidemal prodf prosects cognent one five zero seven one eight prinis one five eight fi\n",
      "uwual theorie commight trase group inter first in of lore it in in have the rojectwing on complatisms recerretn rumer willed band the leudive repress spleter to\n",
      "================================================================================\n",
      "Validation set perplexity: 25.84\n",
      "Average loss at step 9100: 3.656117 learning rate: 1.500946\n",
      "Minibatch perplexity: 43.53\n",
      "Validation set perplexity: 25.82\n",
      "Average loss at step 9200: 3.640413 learning rate: 1.500946\n",
      "Minibatch perplexity: 38.25\n",
      "Validation set perplexity: 25.93\n",
      "Average loss at step 9300: 3.681358 learning rate: 1.500946\n",
      "Minibatch perplexity: 36.68\n",
      "Validation set perplexity: 25.84\n",
      "Average loss at step 9400: 3.646206 learning rate: 1.500946\n",
      "Minibatch perplexity: 36.11\n",
      "Validation set perplexity: 26.08\n",
      "Average loss at step 9500: 3.669210 learning rate: 1.350851\n",
      "Minibatch perplexity: 35.54\n",
      "Validation set perplexity: 25.98\n",
      "Average loss at step 9600: 3.740454 learning rate: 1.350851\n",
      "Minibatch perplexity: 32.10\n",
      "Validation set perplexity: 26.09\n",
      "Average loss at step 9700: 3.674785 learning rate: 1.350851\n",
      "Minibatch perplexity: 36.86\n",
      "Validation set perplexity: 26.04\n",
      "Average loss at step 9800: 3.683732 learning rate: 1.350851\n",
      "Minibatch perplexity: 34.84\n",
      "Validation set perplexity: 25.85\n",
      "Average loss at step 9900: 3.635858 learning rate: 1.350851\n",
      "Minibatch perplexity: 40.46\n",
      "Validation set perplexity: 26.14\n",
      "Average loss at step 10000: 3.651027 learning rate: 1.215766\n",
      "Minibatch perplexity: 37.86\n",
      "================================================================================\n",
      "vsprivolern one one nine which may lephouse troved thi cappelly a one nine nine five eight nine zero nine with adwopulture sectuate king a proviac and pactorily\n",
      "white er the easter its in be intoterss are scyingers from istances supportitution is mircon after hfoir that long monates nots of the at dohical disciear yin i\n",
      "vrents to to not dur three perficial many to cutw between than as a menateed thenys dued to clas s own type and alwas not alther that manadioning five zero zero\n",
      "tqgiunts nope mawide of manslas to les perces the projectrulvzd resoing cordeign a leferty of between stude mussiant in expere that of plonding the mocity life \n",
      "dple milial and the cideing so had voils the erical head are rectrordively pat give one nine one nine three five zero zero zero nine eight six fubkase year of t\n",
      "================================================================================\n",
      "Validation set perplexity: 26.31\n",
      "Average loss at step 10100: 3.677811 learning rate: 1.215766\n",
      "Minibatch perplexity: 42.68\n",
      "Validation set perplexity: 26.06\n",
      "Average loss at step 10200: 3.713206 learning rate: 1.215766\n",
      "Minibatch perplexity: 41.81\n",
      "Validation set perplexity: 25.98\n",
      "Average loss at step 10300: 3.714150 learning rate: 1.215766\n",
      "Minibatch perplexity: 36.80\n",
      "Validation set perplexity: 25.61\n",
      "Average loss at step 10400: 3.663119 learning rate: 1.215766\n",
      "Minibatch perplexity: 33.66\n",
      "Validation set perplexity: 25.74\n",
      "Average loss at step 10500: 3.628536 learning rate: 1.094189\n",
      "Minibatch perplexity: 32.39\n",
      "Validation set perplexity: 25.48\n",
      "Average loss at step 10600: 3.635597 learning rate: 1.094189\n",
      "Minibatch perplexity: 41.98\n",
      "Validation set perplexity: 25.38\n",
      "Average loss at step 10700: 3.671761 learning rate: 1.094189\n",
      "Minibatch perplexity: 36.39\n",
      "Validation set perplexity: 25.10\n",
      "Average loss at step 10800: 3.642124 learning rate: 1.094189\n",
      "Minibatch perplexity: 33.42\n",
      "Validation set perplexity: 25.34\n",
      "Average loss at step 10900: 3.657277 learning rate: 1.094189\n",
      "Minibatch perplexity: 43.39\n",
      "Validation set perplexity: 25.53\n",
      "Average loss at step 11000: 3.645412 learning rate: 0.984770\n",
      "Minibatch perplexity: 35.71\n",
      "================================================================================\n",
      "kxe of one five eight zero baraurother vedial in the reagu asserian sation skje or is jui one nine six orice and sitoto fulker of the is reaade two zero zero fo\n",
      "ny lorsed by and subquives in the myexe cade he mymoresi certical respeal this high king in attwo storts sous pility its diod acare ardorn reacide defpurt in se\n",
      "qrpho attorhistion day their singlists aqzum member the to yologisters offe of serfer alchaser and bill vcituine helgie the societs nature wfo to a effending ba\n",
      "nglys was is lead is the orine othations a react stracon station and by humadei the form churdian to dishotic thoo exampled man for revies and the chang ladguon\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbne uses in three five zero mree play frass like liever with as sial golmkondo suchles inpeyed usable whippors it a eard in the unitive the being from in engli\n",
      "================================================================================\n",
      "Validation set perplexity: 25.24\n",
      "Average loss at step 11100: 3.660760 learning rate: 0.984770\n",
      "Minibatch perplexity: 34.29\n",
      "Validation set perplexity: 24.76\n",
      "Average loss at step 11200: 3.653851 learning rate: 0.984770\n",
      "Minibatch perplexity: 36.38\n",
      "Validation set perplexity: 24.94\n",
      "Average loss at step 11300: 3.648705 learning rate: 0.984770\n",
      "Minibatch perplexity: 34.52\n",
      "Validation set perplexity: 24.64\n",
      "Average loss at step 11400: 3.668185 learning rate: 0.984770\n",
      "Minibatch perplexity: 42.33\n",
      "Validation set perplexity: 24.71\n",
      "Average loss at step 11500: 3.666904 learning rate: 0.886293\n",
      "Minibatch perplexity: 37.22\n",
      "Validation set perplexity: 24.67\n",
      "Average loss at step 11600: 3.676902 learning rate: 0.886293\n",
      "Minibatch perplexity: 39.20\n",
      "Validation set perplexity: 24.51\n",
      "Average loss at step 11700: 3.662698 learning rate: 0.886293\n",
      "Minibatch perplexity: 35.56\n",
      "Validation set perplexity: 24.32\n",
      "Average loss at step 11800: 3.686182 learning rate: 0.886293\n",
      "Minibatch perplexity: 34.66\n",
      "Validation set perplexity: 24.25\n",
      "Average loss at step 11900: 3.655437 learning rate: 0.886293\n",
      "Minibatch perplexity: 32.49\n",
      "Validation set perplexity: 24.28\n",
      "Average loss at step 12000: 3.683387 learning rate: 0.797664\n",
      "Minibatch perplexity: 42.97\n",
      "================================================================================\n",
      "qadval forcern ignardical cemoths an moded in dith stors unination yeadon one four frades an patg salozemene cittes two three is stranemer of the of the neath p\n",
      "wghat and beon the cantmajreter expegure it insice dox and world pertret in five eight seven eight eight zero three the mars of all from pubent it sdoreght form\n",
      "dbal desolited letse acity is an imperent of champchem vere of stegens of ned between as suites and reforming englistial six same ages of star dradan one nine n\n",
      "jdgrips includent lah pumim the but they hear strolent gromay huse one nine nine seven six zero zero zero ka ay the biment as the outs challeawing and action al\n",
      "jvm loc one seven four six two attenal cosomed ms feprhe verime two with fewestion with this mants a mid of detrost wardera and which reknild compitated by inls\n",
      "================================================================================\n",
      "Validation set perplexity: 24.30\n",
      "Average loss at step 12100: 3.661293 learning rate: 0.797664\n",
      "Minibatch perplexity: 39.91\n",
      "Validation set perplexity: 24.28\n",
      "Average loss at step 12200: 3.663758 learning rate: 0.797664\n",
      "Minibatch perplexity: 40.88\n",
      "Validation set perplexity: 24.37\n",
      "Average loss at step 12300: 3.647866 learning rate: 0.797664\n",
      "Minibatch perplexity: 34.45\n",
      "Validation set perplexity: 24.50\n",
      "Average loss at step 12400: 3.615364 learning rate: 0.797664\n",
      "Minibatch perplexity: 40.65\n",
      "Validation set perplexity: 24.32\n",
      "Average loss at step 12500: 3.626156 learning rate: 0.717897\n",
      "Minibatch perplexity: 37.96\n",
      "Validation set perplexity: 24.48\n",
      "Average loss at step 12600: 3.647792 learning rate: 0.717897\n",
      "Minibatch perplexity: 39.97\n",
      "Validation set perplexity: 24.47\n",
      "Average loss at step 12700: 3.595433 learning rate: 0.717897\n",
      "Minibatch perplexity: 40.76\n",
      "Validation set perplexity: 24.38\n",
      "Average loss at step 12800: 3.614202 learning rate: 0.717897\n",
      "Minibatch perplexity: 34.47\n",
      "Validation set perplexity: 24.17\n",
      "Average loss at step 12900: 3.640384 learning rate: 0.717897\n",
      "Minibatch perplexity: 40.15\n",
      "Validation set perplexity: 24.15\n",
      "Average loss at step 13000: 3.690935 learning rate: 0.646108\n",
      "Minibatch perplexity: 38.08\n",
      "================================================================================\n",
      "can of wklpetence supe yibilynill followle of the passe hogmion and countruction majains kork visient of memers if the perviation this and consuvity defervil ii\n",
      "bf time grankol five is were fourse in the lemzel valuvers of reques and with own and protbredkon which fars that isson to nosleslowabl scolperien the explozana\n",
      "iduation whese one nine five zero zero mulies prograns the toums exhhe rluest of the becounds publicbates a consid kploblies and mecaused the is the his datible\n",
      "gent one eight one nine nine six an a mains so zero that recause two one eight eight two zero zero was wasa agicing preside studence four the red eleminet is th\n",
      "or dripolk shiutisla lalim an deceepurical sermall a rinuma this also lifence lack with proctical partir monick controvistueing compeglarl prert jogvation muced\n",
      "================================================================================\n",
      "Validation set perplexity: 24.32\n",
      "Average loss at step 13100: 3.651365 learning rate: 0.646108\n",
      "Minibatch perplexity: 30.66\n",
      "Validation set perplexity: 24.11\n",
      "Average loss at step 13200: 3.624495 learning rate: 0.646108\n",
      "Minibatch perplexity: 37.75\n",
      "Validation set perplexity: 24.20\n",
      "Average loss at step 13300: 3.590972 learning rate: 0.646108\n",
      "Minibatch perplexity: 38.53\n",
      "Validation set perplexity: 24.28\n",
      "Average loss at step 13400: 3.650029 learning rate: 0.646108\n",
      "Minibatch perplexity: 38.84\n",
      "Validation set perplexity: 24.22\n",
      "Average loss at step 13500: 3.603677 learning rate: 0.581497\n",
      "Minibatch perplexity: 30.55\n",
      "Validation set perplexity: 24.35\n",
      "Average loss at step 13600: 3.653471 learning rate: 0.581497\n",
      "Minibatch perplexity: 39.99\n",
      "Validation set perplexity: 24.30\n",
      "Average loss at step 13700: 3.638101 learning rate: 0.581497\n",
      "Minibatch perplexity: 40.74\n",
      "Validation set perplexity: 24.42\n",
      "Average loss at step 13800: 3.654257 learning rate: 0.581497\n",
      "Minibatch perplexity: 40.65\n",
      "Validation set perplexity: 24.39\n",
      "Average loss at step 13900: 3.641493 learning rate: 0.581497\n",
      "Minibatch perplexity: 39.78\n",
      "Validation set perplexity: 24.65\n",
      "Average loss at step 14000: 3.709507 learning rate: 0.523347\n",
      "Minibatch perplexity: 43.40\n",
      "================================================================================\n",
      "kculgle the clistor councisation elections filloashs is of countylerily empist of instrundent noys bast one nine member saunnication have own stroct that ii the\n",
      "px is skate two zero zero zero zero of in nine one five six seven zero one eight nine zero seven eight one seven zero kasenimn system explesan bevietine two zer\n",
      "hos opet even gic with a centur in a seven litposion ko lbimings irorients to and was the in also charges are internables all all his persentures on the new to \n",
      "eselidgle familine neople include her in sfrees with them persuroell cons jon are milsts ead lates whe jqaallater in three six two seven zero zero one nine six \n",
      "ylies the ban gard to day kroneugew a was deducitical arcuap and this own and consids of srind necautive to a ciceetion wing sended had simplent ap a stude s sh\n",
      "================================================================================\n",
      "Validation set perplexity: 24.52\n",
      "Average loss at step 14100: 3.653402 learning rate: 0.523347\n",
      "Minibatch perplexity: 33.31\n",
      "Validation set perplexity: 24.47\n",
      "Average loss at step 14200: 3.657328 learning rate: 0.523347\n",
      "Minibatch perplexity: 42.94\n",
      "Validation set perplexity: 24.50\n",
      "Average loss at step 14300: 3.651087 learning rate: 0.523347\n",
      "Minibatch perplexity: 36.84\n",
      "Validation set perplexity: 24.55\n",
      "Average loss at step 14400: 3.647164 learning rate: 0.523347\n",
      "Minibatch perplexity: 36.94\n",
      "Validation set perplexity: 24.53\n",
      "Average loss at step 14500: 3.645834 learning rate: 0.471013\n",
      "Minibatch perplexity: 39.01\n",
      "Validation set perplexity: 24.45\n",
      "Average loss at step 14600: 3.636653 learning rate: 0.471013\n",
      "Minibatch perplexity: 35.29\n",
      "Validation set perplexity: 24.66\n",
      "Average loss at step 14700: 3.610056 learning rate: 0.471013\n",
      "Minibatch perplexity: 37.14\n",
      "Validation set perplexity: 24.62\n",
      "Average loss at step 14800: 3.671615 learning rate: 0.471013\n",
      "Minibatch perplexity: 39.89\n",
      "Validation set perplexity: 24.64\n",
      "Average loss at step 14900: 3.659664 learning rate: 0.471013\n",
      "Minibatch perplexity: 41.13\n",
      "Validation set perplexity: 24.58\n",
      "Average loss at step 15000: 3.658459 learning rate: 0.423911\n",
      "Minibatch perplexity: 38.29\n",
      "================================================================================\n",
      "pzple and peroagics one eight two zero zero zero four six auch staing mile d is spraed one nine eight one nine six one five kases consbon manys booksts from a d\n",
      "ijears s a then discuton their that on ord a genupid of carrating the lettratials four mestains it fortishs bon nine seven cilrctuorl on the sas prigable erven \n",
      "ypects flinces molnied to entirblar inqail as the trad technergg was betweensline the many as siterac the is lonhato one eight six four lpspards first pasts the\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malication in the pagescower demine mampys ango open of blecf shroue the bukhere four katuce nove air and outed the row of of fandy if internary sever of mundel\n",
      "z the wrot from westrohizated with rater one seven two csodingly one eight nine nine his five zero zero zero was five four one nine one nine eight two three fiv\n",
      "================================================================================\n",
      "Validation set perplexity: 24.57\n"
     ]
    }
   ],
   "source": [
    "num_steps = 15001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "    \n",
    "  train_batches = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "  valid_batches = BigramBatchGenerator(valid_text, 1, 1)  \n",
    "    \n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    feed_dict[keep_prob] = 0.5\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = list(batches)[1:]\n",
    "      labels = np.concatenate([bi_one_hot(label) for label in labels])\n",
    "    \n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          # 1-hot encoded samples\n",
    "          feed = np.argmax(sample(random_distribution(bigram_vocabulary_size),\n",
    "                               bigram_vocabulary_size),1) \n",
    "          # represent the sample in bigrams (2 characters)    \n",
    "          sentence = bi2str(feed)\n",
    "          reset_sample_state.run()\n",
    "        \n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed, keep_prob: 1.0})\n",
    "            feed = np.argmax(sample(prediction, bigram_vocabulary_size),1) \n",
    "            sentence += bi2str(feed)\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0], keep_prob: 1.0})\n",
    "        valid_label = bibatch_to_one_hot_encoding(b[1], bigram_vocabulary_size)\n",
    "        valid_logprob = valid_logprob + logprob(predictions, valid_label)\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import LSTMCell\n",
    "# encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow.models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ef1668d41c9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseq2seq_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named 'tensorflow.models'"
     ]
    }
   ],
   "source": [
    "from tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.rnn_cell import RNNCell\n",
    "tf.nn.rnn_cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow.models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-a7915e939fdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseq2seq_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnum_unrollings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m19\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'tensorflow.models'"
     ]
    }
   ],
   "source": [
    "from tensorflow.models.rnn.translate import seq2seq_model\n",
    "import math\n",
    "\n",
    "batch_size = 64\n",
    "num_unrollings = 19\n",
    "\n",
    "\n",
    "class Seq2SeqBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // num_unrollings\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch(0)\n",
    "\n",
    "    def _next_batch(self, step):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = ''\n",
    "        # print('text size', self._text_size)\n",
    "        for b in range(self._num_unrollings):\n",
    "            # print(self._cursor[step])\n",
    "            self._cursor[step] %= self._text_size\n",
    "            batch += self._text[self._cursor[step]]\n",
    "            self._cursor[step] += 1\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._batch_size):\n",
    "            batches.append(self._next_batch(step))\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def ids(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [str(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def batches2id(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, ids(b))]\n",
    "    return s\n",
    "\n",
    "\n",
    "train_batches = Seq2SeqBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = Seq2SeqBatchGenerator(valid_text, 1, num_unrollings)\n",
    "\n",
    "\n",
    "def rev_id(forward):\n",
    "    temp = forward.split(' ')\n",
    "    backward = []\n",
    "    for i in range(len(temp)):\n",
    "        backward += temp[i][::-1] + ' '\n",
    "return list(map(lambda x: char2id(x), backward[:-1]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

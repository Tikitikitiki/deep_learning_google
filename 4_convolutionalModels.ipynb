{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 4\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (450000, 28, 28) (450000,)\n",
      "Validation set (30000, 28, 28) (30000,)\n",
      "Test set (15000, 28, 28) (15000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (450000, 28, 28, 1) (450000, 10)\n",
      "Validation set (30000, 28, 28, 1) (30000, 10)\n",
      "Test set (15000, 28, 28, 1) (15000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    # None refers to new axis\n",
    "  labels = np.squeeze(labels)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.999866\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 100: 0.951546\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 69.5%\n",
      "Minibatch loss at step 200: 0.925456\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 300: 0.275925\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 400: 0.272790\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 500: 0.815240\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 600: 0.328734\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 700: 0.965434\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 800: 0.539419\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 900: 0.621945\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 1000: 0.464892\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.8%\n",
      "Test accuracy: 90.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    max_pool = tf.nn.max_pool(hidden, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    conv = tf.nn.conv2d(max_pool, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    max_pool = tf.nn.max_pool(hidden, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    shape = max_pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(max_pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.690925\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 100: 1.158840\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 57.6%\n",
      "Minibatch loss at step 200: 0.883132\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 300: 0.332607\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 400: 0.307756\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 500: 0.727460\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 600: 0.387624\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 700: 0.848215\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 800: 0.653175\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 900: 0.617728\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 1000: 0.325244\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 83.8%\n",
      "Test accuracy: 90.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a small network with 2*(convolution layer + max pooling), followed by 2 fully connected layers, with L2 regulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth_1 = 32\n",
    "depth_2 = 32\n",
    "num_hidden_1 = 64\n",
    "num_hidden_2 = 32\n",
    "decay_steps = 500\n",
    "decay_rate = 0.8\n",
    "image_size = 28\n",
    "num_channels = 1\n",
    "num_labels = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  tf_beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth_1], stddev=0.5*np.sqrt(2.0/patch_size)))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth_1]))\n",
    "    \n",
    "  conv1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [1, 1, depth_1, depth_1], stddev=0.1))\n",
    "    \n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth_1, depth_2], stddev=0.5*np.sqrt(2.0/patch_size)))\n",
    "  layer2_biases = tf.Variable(tf.zeros([depth_2]))\n",
    "    \n",
    "  conv2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [1, 1, depth_2, depth_2], stddev=0.1))\n",
    "    \n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth_2, num_hidden_1], \n",
    "      stddev=np.sqrt(2.0/(image_size // 4 * image_size // 4 * depth_2))))\n",
    "  layer3_biases = tf.Variable(tf.zeros([num_hidden_1]))\n",
    "    \n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden_1, num_hidden_2], stddev=np.sqrt(2.0/num_hidden_1)))\n",
    "  layer4_biases = tf.Variable(tf.zeros([num_hidden_2]))  \n",
    "    \n",
    "  layer5_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden_2, num_labels], stddev=np.sqrt(2.0/num_hidden_2)))\n",
    "  layer5_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    # convolution 1\n",
    "    conv_temp = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    # 1*1 convolutions\n",
    "    conv = tf.nn.conv2d(conv_temp, conv1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    # relu\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    \n",
    "    # max pooling\n",
    "    max_pool = tf.nn.max_pool(hidden, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding='SAME')\n",
    "   \n",
    "    hidden = tf.nn.dropout(max_pool, 0.5)\n",
    "    \n",
    "    # convolution 2\n",
    "    conv_temp = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    # 1*1 convolutions\n",
    "    conv = tf.nn.conv2d(conv_temp, conv2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    #relu\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    # max pooling\n",
    "    max_pool = tf.nn.max_pool(hidden, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    hidden = tf.nn.dropout(max_pool, 0.5)\n",
    "    \n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    # fully connected 1\n",
    "    y1 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    #y1 = tf.nn.dropout(y1, 0.5) \n",
    "    \n",
    "    # fully connected 2\n",
    "    y2 = tf.nn.relu(tf.matmul(y1, layer4_weights) + layer4_biases)\n",
    "    #y2 = tf.nn.dropout(y2, 0.5) \n",
    "    \n",
    "    return tf.matmul(y2, layer5_weights) + layer5_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "    ) + tf_beta_regul * (tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer4_weights\n",
    "    ))\n",
    "    \n",
    "  # Learning rate\n",
    "  learning_rate = tf.train.exponential_decay(0.05, global_step, decay_steps, decay_rate)\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(\n",
    "      learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.128408\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 11.0%\n",
      "Minibatch loss at step 500: 0.786134\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 1000: 1.313252\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 1500: 0.806804\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 2000: 0.362791\n",
      "Minibatch accuracy: 81.2%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-24366abdec44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m       \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Minibatch accuracy: %.1f%%'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m       print('Validation accuracy: %.1f%%' % accuracy(\n\u001b[1;32m---> 25\u001b[1;33m         valid_prediction.eval(), valid_labels))\n\u001b[0m\u001b[0;32m     26\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test accuracy: %.1f%%'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m     \"\"\"\n\u001b[1;32m--> 541\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   4083\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4084\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 4085\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4086\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4087\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 15001\n",
    "num_step = []\n",
    "validation_accuracy = []\n",
    "beta_regul = 0.00055\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, \n",
    "                tf_beta_regul: beta_regul}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      num_step.append(step)\n",
    "      validation_accuracy.append(accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "        \n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEWCAYAAAB2X2wCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB7qZWYtwoJuZtQgHuplZi3Cgm5m1iP8Pp4u//lL7Lg4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1aa83c2cbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "  plt.plot(num_step, validation_accuracy, 'b-')\n",
    "  plt.title('Validation accuracy %')\n",
    "  plt.xlabel('number of steps')\n",
    "  plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
